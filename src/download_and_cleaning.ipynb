{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aux Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import scipy.stats\n",
    "\n",
    "def timeseries_to_pandas(dict_timeseries):\n",
    "    \"\"\"\n",
    "    Read the UCR timeseries and convert them to pandas dataframes\n",
    "    \"\"\"\n",
    "\n",
    "    case_id = []\n",
    "    dim_id = []\n",
    "    reading_id = []\n",
    "    value = []\n",
    "    class_id = []\n",
    "\n",
    "    for dim in dict_timeseries.keys():\n",
    "        for case in range(0, len(dict_timeseries[dim])):\n",
    "            num_readings = len(dict_timeseries[dim][case]) - 1\n",
    "            classification = dict_timeseries[dim][case][-1]\n",
    "\n",
    "            case_id.extend([case + 1] * num_readings)\n",
    "            dim_id.extend([dim] * num_readings)\n",
    "            reading_id.extend(range(1, num_readings + 1))\n",
    "            value.extend(list(dict_timeseries[dim][case])[0:-1])\n",
    "            class_id.extend([classification] * num_readings)\n",
    "\n",
    "    df = pd.DataFrame({'case_id': case_id,\n",
    "                       'dim_id': dim_id,\n",
    "                       'reading_id': reading_id,\n",
    "                       'value': value,\n",
    "                       'class_id': class_id})\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def pandas_to_numpy(df):\n",
    "    \"\"\"\n",
    "    Convert the pandas dataframes to numpy arrays of the form (n_samples, n_features, n_timestamps)\n",
    "    \"\"\"\n",
    "\n",
    "    df_as_list = []\n",
    "\n",
    "    for case in df.case_id.unique():\n",
    "        df_case = df.loc[df.case_id == case]\n",
    "        case_as_list = []\n",
    "\n",
    "        for dim in df_case.dim_id.unique():\n",
    "            values = list(df_case.loc[df_case.dim_id == dim, 'value'])\n",
    "            case_as_list.append(values)\n",
    "\n",
    "        df_as_list.append(case_as_list)\n",
    "\n",
    "    response_as_list = list(df[['case_id', 'class_id']].drop_duplicates().class_id)\n",
    "\n",
    "    return np.array(df_as_list), np.array(response_as_list)\n",
    "\n",
    "\n",
    "def ResampleLinear1D(original, targetLen=40):\n",
    "    original = np.array(original, dtype=np.float)\n",
    "    index_arr = np.linspace(0, len(original) - 1, num=targetLen, dtype=np.float)\n",
    "    index_floor = np.array(index_arr, dtype=np.int)  # Round down\n",
    "    index_ceil = index_floor + 1\n",
    "    index_rem = index_arr - index_floor  # Remain\n",
    "\n",
    "    val1 = original[index_floor]\n",
    "    val2 = original[index_ceil % len(original)]\n",
    "    interp = val1 * (1.0 - index_rem) + val2 * index_rem\n",
    "    assert (len(interp) == targetLen)\n",
    "    return interp\n",
    "\n",
    "\n",
    "def calculate_entropy(list_values):\n",
    "    counter_values = Counter(list_values).most_common()\n",
    "    probabilities = [elem[1] / len(list_values) for elem in counter_values]\n",
    "    entropy = scipy.stats.entropy(probabilities)\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def calculate_statistics(list_values):\n",
    "    n5 = np.nanpercentile(list_values, 5)\n",
    "    n25 = np.nanpercentile(list_values, 25)\n",
    "    n75 = np.nanpercentile(list_values, 75)\n",
    "    n95 = np.nanpercentile(list_values, 95)\n",
    "    median = np.nanpercentile(list_values, 50)\n",
    "    mean = np.nanmean(list_values)\n",
    "    std = np.nanstd(list_values)\n",
    "    var = np.nanvar(list_values)\n",
    "    rms = np.nanmean(np.sqrt(list_values ** 2))\n",
    "    return [n5, n25, n75, n95, median, mean, std, var, rms]\n",
    "\n",
    "\n",
    "def calculate_crossings(list_values):\n",
    "    zero_crossing_indices = np.nonzero(np.diff(np.array(list_values) > 0))[0]\n",
    "    no_zero_crossings = len(zero_crossing_indices)\n",
    "    mean_crossing_indices = np.nonzero(np.diff(np.array(list_values) > np.nanmean(list_values)))[0]\n",
    "    no_mean_crossings = len(mean_crossing_indices)\n",
    "    return [no_zero_crossings, no_mean_crossings]\n",
    "\n",
    "\n",
    "def get_features(list_values):\n",
    "    entropy = calculate_entropy(list_values)\n",
    "    crossings = calculate_crossings(list_values)\n",
    "    statistics = calculate_statistics(list_values)\n",
    "    return [entropy] + crossings + statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.auth import HTTPDigestAuth\n",
    "\n",
    "import os\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "\n",
    "#IMPORTANT: must have kaggle.json in the right folder, as detailed in \n",
    "#the 'Authentication' section of https://www.kaggle.com/docs/api\n",
    "#import kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### URL: http://www.timeseriesclassification.com/description.php?Dataset=FingerMovements\n",
    "\n",
    "#Download the EEG Data\n",
    "url = 'http://www.timeseriesclassification.com/Downloads/FingerMovements.zip'\n",
    "resp = requests.get(url)\n",
    "\n",
    "#Create folder structure\n",
    "if not os.path.exists(os.path.join('Datasets_raw', 'FingerMovements')):\n",
    "    os.makedirs(os.path.join('Datasets_raw', 'FingerMovements'))\n",
    "\n",
    "#Unzip the EEG files \n",
    "if resp.status_code == 200:\n",
    "    with ZipFile(BytesIO(resp.content)) as zip_file:\n",
    "        for contained_file in zip_file.namelist():\n",
    "            with open(os.path.join('Datasets_raw', 'FingerMovements', contained_file),  \"wb\") as file:\n",
    "                file.write(zip_file.read(contained_file))\n",
    "else:\n",
    "    print('Unable to download files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import arff\n",
    "\n",
    "#from aux_functions import timeseries_to_pandas\n",
    "#from aux_functions import pandas_to_numpy\n",
    "\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = {}\n",
    "data_test = {}\n",
    "\n",
    "for i in range(1,29):\n",
    "    data_train[i] = arff.loadarff(os.path.join('Datasets_raw', \n",
    "                                               'FingerMovements',\n",
    "                                               f'FingerMovementsDimension{i}_TRAIN.arff'))[0]\n",
    "    data_test[i] = arff.loadarff(os.path.join('Datasets_raw', \n",
    "                                               'FingerMovements',\n",
    "                                               f'FingerMovementsDimension{i}_TEST.arff'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to pandas dataframes\n",
    "df_train = timeseries_to_pandas(data_train)\n",
    "df_test = timeseries_to_pandas(data_test)\n",
    "\n",
    "#Convert to numpy arrays\n",
    "X_train, y_train = pandas_to_numpy(df_train)\n",
    "X_test, y_test = pandas_to_numpy(df_test)\n",
    "\n",
    "#Create folder structure and save the arrays\n",
    "if not os.path.exists(os.path.join('Datasets_clean', 'FingerMovements')):\n",
    "    os.makedirs(os.path.join('Datasets_clean', 'FingerMovements'))\n",
    "\n",
    "np.save('Datasets_clean/FingerMovements/X_train', X_train)\n",
    "np.save('Datasets_clean/FingerMovements/y_train', y_train)\n",
    "np.save('Datasets_clean/FingerMovements/X_test', X_test)\n",
    "np.save('Datasets_clean/FingerMovements/y_test', y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
